/**
 * LLM API í´ë¼ì´ì–¸íŠ¸ ìœ í‹¸ë¦¬í‹°
 */

import { 
  LLMProvider, 
  getLLMConfig, 
  getPrimaryProvider, 
  getFallbackProvider 
} from './llm-config';

export interface LLMRequest {
  prompt: string;
  maxTokens?: number;
  temperature?: number;
}

export interface LLMResponse {
  content: string;
  provider: LLMProvider;
  usage?: {
    promptTokens?: number;
    completionTokens?: number;
    totalTokens?: number;
  };
}

/**
 * OpenAI API í˜¸ì¶œ í•¨ìˆ˜
 */
async function callOpenAI(config: any, request: LLMRequest): Promise<LLMResponse> {
  const response = await fetch(config.apiUrl, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${config.apiKey}`
    },
    body: JSON.stringify({
      model: config.model,
      messages: [{ role: 'user', content: request.prompt }],
      max_tokens: request.maxTokens || 1000,
      temperature: request.temperature || 0.7
    })
  });

  if (!response.ok) {
    throw new Error(`OpenAI API ì˜¤ë¥˜: ${response.status} ${response.statusText}`);
  }

  const data = await response.json();
  
  return {
    content: data.choices[0]?.message?.content || '',
    provider: 'openai',
    usage: {
      promptTokens: data.usage?.prompt_tokens,
      completionTokens: data.usage?.completion_tokens,
      totalTokens: data.usage?.total_tokens
    }
  };
}

/**
 * Gemini API í˜¸ì¶œ í•¨ìˆ˜
 */
async function callGemini(config: any, request: LLMRequest): Promise<LLMResponse> {
  const response = await fetch(`${config.apiUrl}?key=${config.apiKey}`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      contents: [{
        parts: [{ text: request.prompt }]
      }],
      generationConfig: {
        temperature: request.temperature || 0.7,
        maxOutputTokens: request.maxTokens || 1000
      }
    })
  });

  if (!response.ok) {
    throw new Error(`Gemini API ì˜¤ë¥˜: ${response.status} ${response.statusText}`);
  }

  const data = await response.json();
  
  return {
    content: data.candidates[0]?.content?.parts[0]?.text || '',
    provider: 'gemini',
    usage: {
      promptTokens: data.usageMetadata?.promptTokenCount,
      completionTokens: data.usageMetadata?.candidatesTokenCount,
      totalTokens: data.usageMetadata?.totalTokenCount
    }
  };
}

/**
 * Claude API í˜¸ì¶œ í•¨ìˆ˜
 */
async function callClaude(config: any, request: LLMRequest): Promise<LLMResponse> {
  const response = await fetch(config.apiUrl, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'x-api-key': config.apiKey,
      'anthropic-version': '2023-06-01'
    },
    body: JSON.stringify({
      model: config.model,
      max_tokens: request.maxTokens || 1000,
      temperature: request.temperature || 0.7,
      messages: [{ role: 'user', content: request.prompt }]
    })
  });

  if (!response.ok) {
    throw new Error(`Claude API ì˜¤ë¥˜: ${response.status} ${response.statusText}`);
  }

  const data = await response.json();
  
  return {
    content: data.content[0]?.text || '',
    provider: 'claude',
    usage: {
      promptTokens: data.usage?.input_tokens,
      completionTokens: data.usage?.output_tokens,
      totalTokens: (data.usage?.input_tokens || 0) + (data.usage?.output_tokens || 0)
    }
  };
}

/**
 * LLM API í˜¸ì¶œ (fallback ì§€ì›)
 */
export async function callLLM(request: LLMRequest): Promise<LLMResponse> {
  const primaryProvider = getPrimaryProvider();
  const fallbackProvider = getFallbackProvider();
  
  console.log(`ğŸ¤– LLM í˜¸ì¶œ - Primary: ${primaryProvider}, Fallback: ${fallbackProvider || 'none'}`);
  
  try {
    // Primary provider ì‹œë„
    const primaryConfig = getLLMConfig(primaryProvider);
    if (!primaryConfig?.apiKey) {
      throw new Error(`${primaryProvider} API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.`);
    }
    
    const response = await callLLMWithProvider(primaryProvider, primaryConfig, request);
    console.log(`âœ… ${primaryProvider} API í˜¸ì¶œ ì„±ê³µ`);
    return response;
    
  } catch (error) {
    console.error(`âŒ ${primaryProvider} API í˜¸ì¶œ ì‹¤íŒ¨:`, error);
    
    // Fallback provider ì‹œë„
    if (fallbackProvider) {
      try {
        const fallbackConfig = getLLMConfig(fallbackProvider);
        if (!fallbackConfig?.apiKey) {
          throw new Error(`${fallbackProvider} API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.`);
        }
        
        const response = await callLLMWithProvider(fallbackProvider, fallbackConfig, request);
        console.log(`âœ… ${fallbackProvider} API í˜¸ì¶œ ì„±ê³µ (fallback)`);
        return response;
        
      } catch (fallbackError) {
        console.error(`âŒ ${fallbackProvider} API í˜¸ì¶œë„ ì‹¤íŒ¨:`, fallbackError);
        throw new Error(`ëª¨ë“  LLM API í˜¸ì¶œ ì‹¤íŒ¨. Primary: ${error}, Fallback: ${fallbackError}`);
      }
    } else {
      throw error;
    }
  }
}

/**
 * íŠ¹ì • providerë¡œ LLM API í˜¸ì¶œ
 */
async function callLLMWithProvider(
  provider: LLMProvider, 
  config: any, 
  request: LLMRequest
): Promise<LLMResponse> {
  switch (provider) {
    case 'openai':
      return callOpenAI(config, request);
    case 'gemini':
      return callGemini(config, request);
    case 'claude':
      return callClaude(config, request);
    default:
      throw new Error(`ì§€ì›ë˜ì§€ ì•ŠëŠ” LLM provider: ${provider}`);
  }
}
