/**
 * LLM API í´ë¼ì´ì–¸íŠ¸ ìœ í‹¸ë¦¬í‹°
 */

import {
  LLMProvider,
  LLMConfig,
  getLLMConfig,
  getPrimaryProvider,
  getFallbackProvider,
  getAvailableProviders
} from './llm-config';

export interface LLMRequest {
  prompt: string;
  maxTokens?: number;
  temperature?: number;
}

export interface LLMResponse {
  content: string;
  provider: LLMProvider;
  usage?: {
    promptTokens?: number;
    completionTokens?: number;
    totalTokens?: number;
  };
}

// API ì‘ë‹µ íƒ€ì… ì •ì˜
interface OpenAIResponse {
  choices: Array<{
    message: {
      content: string;
    };
  }>;
  usage?: {
    prompt_tokens?: number;
    completion_tokens?: number;
    total_tokens?: number;
  };
}

interface GeminiResponse {
  candidates: Array<{
    content: {
      parts: Array<{
        text: string;
      }>;
    };
  }>;
  usageMetadata?: {
    promptTokenCount?: number;
    candidatesTokenCount?: number;
    totalTokenCount?: number;
  };
}

interface ClaudeResponse {
  content: Array<{
    text: string;
  }>;
  usage?: {
    input_tokens?: number;
    output_tokens?: number;
  };
}

/**
 * OpenAI API í˜¸ì¶œ í•¨ìˆ˜
 */
async function callOpenAI(config: LLMConfig, request: LLMRequest): Promise<LLMResponse> {
  const response = await fetch(config.apiUrl, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${config.apiKey}`
    },
    body: JSON.stringify({
      model: config.model,
      messages: [{ role: 'user', content: request.prompt }],
      max_tokens: request.maxTokens || 2000,
      temperature: request.temperature || 0.7
    })
  });

  if (!response.ok) {
    throw new Error(`OpenAI API ì˜¤ë¥˜: ${response.status} ${response.statusText}`);
  }

  const data: OpenAIResponse = await response.json();

  return {
    content: data.choices[0]?.message?.content || '',
    provider: 'openai',
    usage: {
      promptTokens: data.usage?.prompt_tokens,
      completionTokens: data.usage?.completion_tokens,
      totalTokens: data.usage?.total_tokens
    }
  };
}

/**
 * Gemini API í˜¸ì¶œ í•¨ìˆ˜
 */
async function callGemini(config: LLMConfig, request: LLMRequest): Promise<LLMResponse> {
  const response = await fetch(`${config.apiUrl}?key=${config.apiKey}`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      contents: [{
        parts: [{ text: request.prompt }]
      }],
      generationConfig: {
        temperature: request.temperature || 0.7,
        maxOutputTokens: request.maxTokens || 2000
      }
    })
  });

  if (!response.ok) {
    throw new Error(`Gemini API ì˜¤ë¥˜: ${response.status} ${response.statusText}`);
  }

  const data: GeminiResponse = await response.json();

  return {
    content: data.candidates[0]?.content?.parts[0]?.text || '',
    provider: 'gemini',
    usage: {
      promptTokens: data.usageMetadata?.promptTokenCount,
      completionTokens: data.usageMetadata?.candidatesTokenCount,
      totalTokens: data.usageMetadata?.totalTokenCount
    }
  };
}

/**
 * Claude API í˜¸ì¶œ í•¨ìˆ˜
 */
async function callClaude(config: LLMConfig, request: LLMRequest): Promise<LLMResponse> {
  const response = await fetch(config.apiUrl, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'x-api-key': config.apiKey,
      'anthropic-version': '2023-06-01'
    },
    body: JSON.stringify({
      model: config.model,
      max_tokens: request.maxTokens || 2000,
      temperature: request.temperature || 0.7,
      messages: [{ role: 'user', content: request.prompt }]
    })
  });

  if (!response.ok) {
    throw new Error(`Claude API ì˜¤ë¥˜: ${response.status} ${response.statusText}`);
  }

  const data: ClaudeResponse = await response.json();

  return {
    content: data.content[0]?.text || '',
    provider: 'claude',
    usage: {
      promptTokens: data.usage?.input_tokens,
      completionTokens: data.usage?.output_tokens,
      totalTokens: (data.usage?.input_tokens || 0) + (data.usage?.output_tokens || 0)
    }
  };
}

/**
 * LLM API í˜¸ì¶œ (fallback ì§€ì›)
 */
export async function callLLM(request: LLMRequest): Promise<LLMResponse> {
  const primaryProvider = getPrimaryProvider();
  const fallbackProvider = getFallbackProvider();
  const availableProviders = getAvailableProviders();

  console.log(`ğŸ¤– LLM í˜¸ì¶œ - ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡œë°”ì´ë”: ${availableProviders.join(', ')}`);

  try {
    // Primary provider ì‹œë„
    const primaryConfig = getLLMConfig(primaryProvider);
    if (!primaryConfig?.apiKey) {
      throw new Error(`${primaryProvider} API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ`);
    }

    const response = await callLLMWithProvider(primaryProvider, primaryConfig, request);
    console.log(`âœ… ${primaryProvider} API í˜¸ì¶œ ì„±ê³µ`);
    return response;

  } catch (error) {
    // Primary provider ì‹¤íŒ¨ ì‹œ fallback ì‹œë„
    if (fallbackProvider) {
      console.log(`âš ï¸  ${primaryProvider} ì‚¬ìš© ë¶ˆê°€, ${fallbackProvider}ë¡œ ì „í™˜ ì¤‘...`);

      try {
        const fallbackConfig = getLLMConfig(fallbackProvider);
        if (!fallbackConfig?.apiKey) {
          throw new Error(`${fallbackProvider} API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ`);
        }

        const response = await callLLMWithProvider(fallbackProvider, fallbackConfig, request);
        console.log(`âœ… ${fallbackProvider} API í˜¸ì¶œ ì„±ê³µ (ëŒ€ì²´ í”„ë¡œë°”ì´ë”)`);
        return response;

      } catch (fallbackError) {
        console.error(`âŒ ${fallbackProvider} API í˜¸ì¶œë„ ì‹¤íŒ¨:`, fallbackError);
        throw new Error(`ëª¨ë“  LLM API í˜¸ì¶œ ì‹¤íŒ¨. Primary: ${error}, Fallback: ${fallbackError}`);
      }
    } else {
      console.error(`âŒ ${primaryProvider} API í˜¸ì¶œ ì‹¤íŒ¨, ì‚¬ìš© ê°€ëŠ¥í•œ ëŒ€ì²´ í”„ë¡œë°”ì´ë” ì—†ìŒ:`, error);
      throw error;
    }
  }
}

/**
 * íŠ¹ì • providerë¡œ LLM API í˜¸ì¶œ
 */
async function callLLMWithProvider(
  provider: LLMProvider,
  config: LLMConfig,
  request: LLMRequest
): Promise<LLMResponse> {
  switch (provider) {
    case 'openai':
      return callOpenAI(config, request);
    case 'gemini':
      return callGemini(config, request);
    case 'claude':
      return callClaude(config, request);
    default:
      throw new Error(`ì§€ì›ë˜ì§€ ì•ŠëŠ” LLM provider: ${provider}`);
  }
}
